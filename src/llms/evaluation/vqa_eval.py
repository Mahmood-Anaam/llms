import re
import json
from collections import defaultdict
import math

class ArabicVQAEval:
    def __init__(self, ground_truth, predictions, n=2):
        """
        Initializes the evaluation class.

        Args:
            ground_truth (dict): Ground truth data containing question IDs and their corresponding answers.
            predictions (dict): Predicted answers generated by the model.
            n (int): Precision for rounding the accuracy.
        """
        self.n = n
        self.ground_truth = ground_truth  # Expected format: {"question_id": ["answer1", "answer2", ...]}
        self.predictions = predictions    # Expected format: {"question_id": "predicted_answer"}
        self.accuracy = {}
        self.evalQA = {}
        self.evalQuesType = defaultdict(list)
        self.evalAnsType = defaultdict(list)

        # Arabic-specific processing rules
        self.manual_map = {
            "واحد": "1", "اثنان": "2", "ثلاثة": "3", "أربعة": "4", "خمسة": "5",
            "ستة": "6", "سبعة": "7", "ثمانية": "8", "تسعة": "9", "عشرة": "10"
        }
        self.articles = ["ال", "و"]  # Remove articles
        self.punct = [";", "/", "[", "]", "\"", "{", "}", "(", ")", "=", "_", "-", "<", ">", "@", "؟", "!"]

    def process_punctuation(self, text):
        """
        Cleans and normalizes punctuation in text.

        Args:
            text (str): Input text to process.

        Returns:
            str: Processed text.
        """
        text = text.lower()
        for punct in self.punct:
            text = text.replace(punct, "")
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def process_digit_and_article(self, text):
        """
        Normalizes digits and removes articles in Arabic text.

        Args:
            text (str): Input text to process.

        Returns:
            str: Processed text.
        """
        words = text.split()
        normalized_words = [self.manual_map.get(word, word) for word in words if word not in self.articles]
        return ' '.join(normalized_words)

    def compute_bleu(self, reference, hypothesis):
        """
        Computes the BLEU score for a single reference and hypothesis.

        Args:
            reference (str): Ground truth answer.
            hypothesis (str): Predicted answer.

        Returns:
            float: BLEU score.
        """
        def n_grams(sequence, n):
            return [tuple(sequence[i:i + n]) for i in range(len(sequence) - n + 1)]

        reference_tokens = reference.split()
        hypothesis_tokens = hypothesis.split()

        precisions = []
        for n in range(1, 5):
            ref_ngrams = n_grams(reference_tokens, n)
            hyp_ngrams = n_grams(hypothesis_tokens, n)

            if len(hyp_ngrams) == 0:
                precisions.append(0)
                continue

            matching_ngrams = len(set(hyp_ngrams) & set(ref_ngrams))
            precisions.append(matching_ngrams / len(hyp_ngrams))

        if all(p == 0 for p in precisions):
            return 0

        brevity_penalty = min(1.0, len(hypothesis_tokens) / len(reference_tokens))
        bleu = brevity_penalty * math.exp(sum(math.log(p) for p in precisions if p > 0) / len(precisions))
        return bleu

    def evaluate(self):
        """
        Evaluates the model predictions against the ground truth answers.
        """
        accuracies = []
        bleu_scores = []

        for question_id, gt_answers in self.ground_truth.items():
            if question_id not in self.predictions:
                continue

            pred_answer = self.predictions[question_id]
            pred_answer = self.process_punctuation(pred_answer)
            pred_answer = self.process_digit_and_article(pred_answer)

            gt_answers = [self.process_punctuation(ans) for ans in gt_answers]
            gt_answers = [self.process_digit_and_article(ans) for ans in gt_answers]

            # Compute accuracy for the question
            gt_acc = []
            for gt_answer in gt_answers:
                matching_answers = [1 for ans in gt_answers if ans == pred_answer]
                accuracy = min(1.0, sum(matching_answers) / 3.0)
                gt_acc.append(accuracy)

            question_accuracy = sum(gt_acc) / len(gt_acc) if gt_acc else 0
            self.evalQA[question_id] = round(100 * question_accuracy, self.n)
            accuracies.append(question_accuracy)

            # Compute BLEU scores
            bleu = max(self.compute_bleu(ref, pred_answer) for ref in gt_answers)
            bleu_scores.append(bleu)

        # Compute overall accuracy and BLEU
        self.accuracy['overall'] = round(100 * sum(accuracies) / len(accuracies), self.n) if accuracies else 0
        self.accuracy['bleu'] = round(100 * sum(bleu_scores) / len(bleu_scores), self.n) if bleu_scores else 0

    def save_results(self, output_path):
        """
        Saves evaluation results to a JSON file.

        Args:
            output_path (str): Path to save the evaluation results.
        """
        results = {
            "accuracy": self.accuracy,
            "evalQA": self.evalQA,
        }
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)

# Example usage
ground_truth = {
    "1": ["الرياض", "مدينة الرياض", "الرياض عاصمة السعودية"],
    "2": ["أسد", "الأسد", "حيوان الأسد"]
}
predictions = {
    "1": "الرياض",
    "2": "الأسد"
}

evaluator = ArabicVQAEval(ground_truth, predictions)
evaluator.evaluate()
print("Overall Accuracy:", evaluator.accuracy['overall'])
print("Overall BLEU Score:", evaluator.accuracy['bleu'])
print("Per Question Accuracy:", evaluator.evalQA)

evaluator.save_results("evaluation_results.json")